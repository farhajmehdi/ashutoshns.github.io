<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"> 
		<link href="https://fonts.googleapis.com/css?family=EB+Garamond" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet"> 
		<style type="text/css">
			body{font-family: 'Raleway', serif;}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-style: normal;font-variant: normal;font-weight: 800;font-size: 36px;padding: 1%;}
			.section{position: relative;width: 90%;clear: left;margin: auto;padding: 2%;}
			.subsection{position: relative;clear: left; width: 90%;text-align: justify;padding: 10px;}
			.heading{position: relative; width: 98%;text-align: left;font-style: normal;font-variant: normal;font-size: 24px;font-weight: 500;}
			.headingss{position: relative; width: 98%;text-align: left;font-weight: 500;font-style: bold;font-size: 20px;font-weight: medium;}
			.text{width: 95%;font-size: 17px;font-style: thin;font-weight: 10;font-variant: normal;text-align: justify;padding: 10px 0px 10px 0px;line-height: 1.5}
			.authors{position: relative;width: 90%;margin: auto;padding: 2%;font-style: thin;font-variant: normal;text-align: center;font-size: 13px;}
			.image{width: 90%;font-size: 15px;text-align: center;}
		</style>
	</head>
	<body>
		<div class="container">
			
			<div class="title">URBAN ENVIRONMENT AUDIO CLASSIFICATION</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>Ashutosh Agrawal, Roll No.: 150102007, Branch: ECE</p>; &nbsp; &nbsp;
				<p> Ashutosh Sharma, Roll No.: 150102008, Branch: ECE</p>; &nbsp; &nbsp;<br>
				<p>   Rahul Khamkar, Roll No.: 150102027, Branch: ECE</p>; &nbsp;&nbsp; 
				<p> Anubhav Agrawal, Roll No.: 150108004, Branch: EEE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->
				
			</div>
			<hr>
			
			<div class="section">
				<div class="heading">Abstract</div>
				<div class="text container">

					<!-- Start edit here  -->
					The goal of this project is to build an audio classifier capable of recognising different sounds. The sounds we have used are ambient noises from an urban environment. Right now we have focused on 4 categories of sounds. This can prove to be a very effective tool for audio surviellance of the sorrounding. It's applications can vary from as simple as making a device to detect a dog's bark to as big as detection of gun shots/bombing or other terrorist activities. We tried plotting different types of spectrograms of samples from each class which inturn lead us to see what filters would be best suited for this classification. In the end we used Mel-frequency cepstral coefficients, Mel-scaled power spectrogram, Chromagram of a short-time Fourier transform, Octave-based spectral contrast, Tonnetz to extract features. The coeffecients from these filters led to 193 features which were then used for classification using Machine Learning.					
					<!-- Stop edit here -->

				</div>
			</div>
			
			<div class="section">
				<div class="heading">1. Introduction</div>
				<div class="text container">

					<!-- Start edit here  -->
					As mentioned, the goal of this project is to build an audio classifier capable of recognising different sounds that are heard in our environment. In this section, I would introduce you to the problem statement and the try to explain the challenges it posed, then present some litreature survey that we did and lastly explaining the approach that we decieded to take for solving this problem.
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="headingss">1.1 Introduction to Problem</div>
					<div class="text container">

						<!-- Start edit here  -->
						The crux of the problem is this: given a never-before-heard recording, how can a system be trained to identify what it is actually listening to?</div>
						<div class="text container">Let’s try to refine this problem. The input format will be the digital representation of the original analogue waveforms, just as would be recorded by a microphone, encoded using pulse-code modulation and stored as (lossless) WAV files.</div>
						<div class="text container">From this input, we’ll need to extract features. But sound recordings are not spreadsheets, with their data neatly organised into rows and columns of known significance. Say we sample two recordings of equal duration, creating 2000 discrete floating point numbers; we can not meaningfully compare the 1000th number of one recording with the 1000th number of another. This is because we have no way of telling whether the number at any particular point in time is signal, silence or noise. Instead, we must abstract away from individual numbers, and consider data values in the context of many of its neighbours. This will grant us the ability to start spotting patterns which we call features, and permit us to generalise - the fundamental requirement in machine learning.</div>
						<div class="text container">The challenge then, is to find measurable properties that differ in dissimilar recordings and are alike in those from similar sources. The complexity of feature extraction makes this problem an attractive application of deep learning, whose hierarchical nature makes it capable of automated feature learning. Once features have somehow been extracted, the question becomes how can we use them to train a model, and then use what we’ve learned to generate predictions?</div>
						<div class="text container">Next, we’ll want to tune the model produced, to achieve the best possible accuracy. So we’ll need to consider how we can measure successful classification - and how can the performance of the model be optimised. Putting all this together suggests a processing pipeline as shown in Fig 1.
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="headingss">1.2 Figure</div>
					<div class="image container">

						<!-- Start edit here  -->
						<div style="width: 85%; float: left;">
							<p>
								<img src="img/fig1.jpg" alt="Overview of the project" width="100%" align="middle"/>
								Fig. 1: An overview block diagram of the project.
							</p>
						</div>
						<!--div style="width:45%; float:left">
							<p>&nbsp;&nbsp;&nbsp;&nbsp;
								<img src="img/fig2.jpg" alt="Amplitude vs. Time plot for sample files from all 4 categories." style="width:90%; height:auto; align:center;">
								Fig 2: Amplitude vs. Time plot for sample files from all four categories.
							</p>
						</div>
						<div style="width:45%; float:left">
							<p>&nbsp;&nbsp;&nbsp;&nbsp;
								<img src="img/fig3.jpg" alt="Spectrogram of sample files from all 4 categories." style="width:90%; height:auto; align:center;">
								Fig 3: Spectrogram of sample files from all four categories.
							</p>
						</div-->
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						<ul>
							<li>Urmila Shrawankar and Vilas Thakare, "Techniques for Feature Extraction in Speech Recognition System: A Comparative Study", International Journal of Computer Applications in Engineering, Technology and Sciences (IJCAETS), pp. 412-418, 2010.</li>
							<li>Selina Chu, Shrikanth Narayanan, and CC Jay Kuo, “Environmental sound recognition with time–frequency audio features,” IEEE Transactions on Audio, Speech, and Language Processing,vol. 17, no. 6, pp. 1142–1158, 2009.</li>
							<li>K. J. Piczak, “Environmental sound classification with convolutional neural networks,” in 25th International Workshop on Machine Learning for Signal Processing (MLSP), Boston, MA, USA, Sep. 2015, pp. 1–6</li>
						</ul>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">1.4 Proposed Approach Idea</div>
					<div class="text">

						<!-- Start edit here  -->
						Here is the crude basic idea:
						<ul>
							<li>Using different frequency scales, explore spectrograms.</li>
							<li>Using this, determine what filter banks are ideal for the purpose</li>
							<li>Use these filter banks to generate features</li>
							<li>Use algorithmic training and these features to automate classifier design</li>
							<li>Done. Now test this on new data to calculate efficiency.</li>
						</ul>
						<!-- Stop edit here -->

					</div>
				</div>
			</div>
				
			<div class="section">
				<div class="heading">1. Proposed Approach</div>
				<div class="text container">

					<!-- Start edit here  -->
					In this section we explain the proposed approach of the enitre project in detail. Part 1.1 about exploration is not quite a part of 'how we solve this problem' but a very integral part of 'how we decide how to solve this problem' so we decide to include it.
					<!-- Stop edit here -->

				</div>
				
				
				<div class="subsection">
					<div class="headingss">2.1 Exploratory</div>
					<div class="text">
					Once in memory, a common visualisation for audio recordings is the waveform plot, which depicts the amplitude (relative loudness) of the sound at each successive time interval, this is what you'll see if you load an audio file into a sound editor like Audacity. Here are the waveforms for a randomly chosen example of each of the dataset’s 4 classes:
					</div>
				</div>
				<div style="width:45%; float:left">
					<p>&nbsp;&nbsp;&nbsp;&nbsp;
						<img src="img/fig2.jpg" alt="Amplitude vs. Time plot for sample files from all 4 categories." style="width:90%; height:auto; align:center;">
						Fig 2: Amplitude vs. Time plot for sample files from all four categories.
					</p>
				</div>
				<div style="width:45%; float:left">
					<p>&nbsp;&nbsp;&nbsp;&nbsp;
						<img src="img/fig3.jpg" alt="Spectrogram of sample files from all 4 categories." style="width:90%; height:auto; align:center;">
						Fig 3: Spectrogram of sample files from all four categories.
					</p>
				</div>
				<div class="text" style="clear: left">
					In these plots, time is on the horizontal axis and amplitude on the vertical. We can see that some of the samples, like a dog barking, have a shape that is distinctively different from the others, which our classifier might be able to utilise to distinguish between them. Others like air-conditioning and engine idling are superficially more similar, and might be more difficult to tell apart - even for a human listener.
				</div>
				<div class="text">
					Matplotlib provides an alternative visualisation method called spectrogram that calculates and plots the different intensities of the frequency spectrum. This creates a different depiction of each sound:
				</div>
				
				<div class="text" style="clear: left">
					Visually, it seems slightly easier to distinguish between the different classes with this visualisation, and indeed the intensities of frequencies will be the basis of the features I intend to extract. The notebook contains a variation of this visualisation using Librosa’s log power spectrogram plotting.
				</div>
				<div class=""text>
					Following are visualizations of other features in the order air conditioner, car horn, street music and children playing.
				</div>
				
				<div style="width: 25%; float: left;">
					<p>
						air conditioner
						<img src="img/aircon_mel.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						car horn
						<img src="img/carhorn_mel.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						street music
						<img src="img/music_mel.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						children playing
						<img src="img/play_mel.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/aircon.wav_mfcc.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/carhorn.wav_mfcc.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/music.wav_mfcc.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/play.wav_mfcc.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/aircon.wav_spec.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/carhorn.wav_spec.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/music.wav_spec.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/play.wav_spec.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/aircon.wav_chroma.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/carhorn.wav_chroma.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/music.wav_chroma.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/play.wav_chroma.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/aircon.wav_ton.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/carhorn.wav_ton.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/music.wav_ton.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div style="width: 25%; float: left;">
					<p>
						<img src="img/play.wav_ton.png" alt="image of a feature" width="100%" align="middle"/>
					</p>
				</div>
				<div class="text">
					These plots illustrate the complexity of the data we need to process. In typical supervised learning projects, every example in the training set has the same features, providing comparable values. But this is not the case with media such as audio or images, because any two files will not contain a pair of values that are semantically equivalent. Even if two audio samples had the same size, we can’t meaningfully compare the respective value at an arbitrary point in time. It would be like an image classifier trying to distinguish between two photos by comparing the pixels found in each at the same arbitrary location.
				</div>
				<div class="text">
					Consequently, in audio processing we can not consider each data value in isolation, and can not determine what is signal and what is noise merely by looking at the quantiles of a frequency distribution. Instead, the salient information is the patterns of values, spread over wide regions of the recording. This means we must consider how the signal changes over time, and what patterns can be identified. How this can be achieved will be discussed in section 2.3.
				</div>
				
				
				<div class="subsection">
					<div class="headingss">2.2 Feature extraction </div>
					<div class="text container">

						The classification model will be fed with features extracted from our audio dataset, based on which it will be trained to make a decision.All of the audio clips collected have different sampling rates. The librosa library that we used uniformly samples the clips at a frequency of 22KHz. This means that in the time domain, each second of each audio clip input has 22,000 data points. We cannot directly feed this data to train our model, because there may be many redundant features here which will harm our model's accuracy.
					<div class="text container">So we extract typical frequency domain features from these clips. We selected the features to be used here based on the results of similar studies published in the past (check the literature review section). The features we used are-
					<ul>
					<li><a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-frequency cepstral coefficients (MFCC)</a> - the coefficients that collectively make up the short-term power spectrum of a sound</li>
					<li><a href="https://en.wikipedia.org/wiki/Mel_scale">Mel-scaled</a> power spectrogram - the Mel Scale is used to provide greater resolution for more informative (lower) frequencies</li>
					<li></a href="https://labrosa.ee.columbia.edu/matlab/chroma-ansyn/">Chromagram</a> of a short-time Fourier transform - projects into bins representing the 12 distinct semitones (or chroma) of the musical octave</li>
					<li><a href="http://ieeexplore.ieee.org/document/1035731/?reload=true">Octave-based spectral contrast</a> - distributions of sound energy over octave frequencies
					</li>
					<li><a href="https://sites.google.com/site/tonalintervalspace/">Tonnetz</a> - estimates tonal centroids as coordinates in a six-dimensional interval space</li>
					</ul>
					The results of these 5 feature extractions are concatenated to give us a 193-dimensional vector for each audio clip irrespective of its length.</div>
					<div class="text container">These featues were extracted using the librosa library in python and were stored as numpy arrays. These arrays were then separately used to train our Feed-Forward Neural Network(FFN) model to classify the audio.</div>

					</div>
				</div>
	
				<div class="subsection">
					<div class="headingss">2.3 Training</div>
					<div class="text">
						<!--Text here-->
						Code Run:<br>
						 <iframe src="train.html" height="50%" width="100%"></iframe> 
						
						
					</div>
				</div>
	
				<div class="subsection">
					<div class="headingss">2.4 Prediction</div>
					<div class="text">
						<!--Text here-->
						Code Run:<br>
						 <iframe src="predict.html" height="50%" width="100%"></iframe> 
					</div>
				</div>
				
			</div>

			<div class="section">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="headingss">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
					
					<div class="subsection">
						<div class="headingss">3.2.1 Results</div>
						<div class="text">
							<!--Text here-->
						</div>
					</div>
					
					<div class="subsection">
						<div class="headingss">3.2.2 Optimization</div>
						<div class="text">
							<!--Text here-->
							Code Run: <a herf="optimizer.html">Click here</a>
						 	 <!--iframe src="https://ashutoshns.github.io/dsp-s5/optimizer.html" height="50%" width="100%"></iframe--> 
						</div>
					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="headingss">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="headingss">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>
			
			
			
	</body>
</html>  
